''.join
(
random.choice
(
string.ascii_uppercase
+
string.digits
)
for
_
in
range
(
N
)
)
print
(
datetime.datetime.now
(
)
.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
'
)
)
datetime.datetime.combine
(
datetime.date.today
(
)
,
datetime.timedelta
(
days=12
)
)
from
datetime
import
datetime
datetime.utcnow
(
)
writer.writerow
(
write
)
for
dirname
,
dirnames
,
filenames
in
os.walk
(
'file.txt
'
)
:
for
subdirname
in
dirnames
:
print
(
os.path.join
(
dirname
,
subdirname
)
)
for
filename
in
filenames
:
pass
re.sub
(
'
#
(
\\w+
)
'
,
'\\1
'
,
text
)
pandas.read_csv
(
file
,
sep='\t
'
,
lineterminator='\r
'
)
df.groupby
(
np.arange
(
len
(
df.columns
)
)
//
2
+
1
,
axis=1
)
.mean
(
)
In
[
479
]
:
df
Out
[
479
]
:
ID
birthyear
weight
0
619040
1962
0.123123
1
600161
1963
0.981742
2
25602033
1963
1.312312
3
624870
1987
0.942120
In
[
480
]
:
df
[
``
weight
''
]
.mean
(
)
Out
[
480
]
:
0.83982437500000007
pd.concat
(
[
df1
,
df2
]
,
axis=1
)
In
[
1
]
:
s1
=
pd.Series
(
[
1
,
2
]
,
index=
[
'
A
'
,
'
B
'
]
,
name='s1
'
)
In
[
2
]
:
s2
=
pd.Series
(
[
3
,
4
]
,
index=
[
'
A
'
,
'
B
'
]
,
name='s2
'
)
In
[
3
]
:
pd.concat
(
[
s1
,
s2
]
,
axis=1
)
Out
[
3
]
:
s1
s2
A
1
3
B
2
4
In
[
4
]
:
pd.concat
(
[
s1
,
s2
]
,
axis=1
)
.reset_index
(
)
Out
[
4
]
:
index
s1
s2
0
A
1
3
1
B
2
4
df
=
df.rename
(
columns=
{
'oldName1
'
:
'newName1
'
,
'oldName2
'
:
'newName2
'
}
)
#
Or
rename
the
existing
DataFrame
(
rather
than
creating
a
copy
)
df.rename
(
columns=
{
'oldName1
'
:
'newName1
'
,
'oldName2
'
:
'newName2
'
}
,
inplace=True
)
df2
=
df.set_axis
(
[
'
V
'
,
'
W
'
,
'
X
'
,
'
Y
'
,
'
Z
'
]
,
axis=1
,
inplace=False
)
df2
V
W
X
Y
Z
0
x
x
x
x
x
1
x
x
x
x
x
2
x
x
x
x
x
df.to_csv
(
file_name
,
sep='\t
'
)
import
pandas
as
pd
pd.options.display.float_format
=
'
$
{
:
,.2f
}
'.format
df
=
pd.DataFrame
(
[
123.4567
,
234.5678
,
345.6789
,
456.7890
]
,
index=
[
'foo
'
,
'bar
'
,
'baz
'
,
'quux
'
]
,
columns=
[
'cost
'
]
)
print
(
df
)
with
open
(
'foo.csv
'
,
'
w
'
)
as
f
:
data.to_csv
(
f
,
index=True
,
header=True
,
decimal=
'
,
'
,
sep=
'
'
,
float_format=
'
%
.3f
'
)
bins
=
[
0
,
1
,
5
,
10
,
25
,
50
,
100
]
df
[
'binned
'
]
=
pd.cut
(
df
[
'percentage
'
]
,
bins
)
print
(
df
)
percentage
binned
0
46.50
(
25
,
50
]
1
44.20
(
25
,
50
]
2
100.00
(
50
,
100
]
3
42.12
(
25
,
50
]
df.loc
[
df
[
'column_name
'
]
==
some_value
]
In
[
11
]
:
df.loc
[
df
[
'col1
'
]
>
=
1
,
'col1
'
]
Out
[
11
]
:
1
1
2
2
Name
:
col1
In
[
12
]
:
df
[
df
[
'col1
'
]
>
=
1
]
Out
[
12
]
:
col1
col2
1
1
11
2
2
12
In
[
13
]
:
df
[
(
df
[
'col1
'
]
>
=
1
)
&
(
df
[
'col1
'
]
<
=1
)
]
Out
[
13
]
:
col1
col2
1
1
11
output
=
df.to_string
(
formatters=
{
'var1
'
:
'
{
:
,.2f
}
'.format
,
'var2
'
:
'
{
:
,.2f
}
'.format
,
'var3
'
:
'
{
:
,.2
%
}
'.format
}
)
print
(
output
)
df
[
'race_label
'
]
=
df.apply
(
lambda
row
:
label_race
(
row
)
,
axis=1
)
LogisticRegression
(
multi_class='multinomial
'
,
solver
='newton-cg
'
)
.fit
(
X_train
,
y_train
)
from
sklearn.ensemble
import
RandomForestClassifier
from
sklearn.model_selection
import
cross_val_score
import
numpy
as
np
clf
=
RandomForestClassifier
(
)
#
Initialize
with
whatever
parameters
you
want
to
#
10-Fold
Cross
validation
print
np.mean
(
cross_val_score
(
clf
,
X_train
,
y_train
,
cv=10
)
)
import
warnings
warnings.filterwarnings
(
'ignore
'
)
plt.savefig
(
'plt
'
,
dpi=1000
)
dates
=
matplotlib.dates.date2num
(
list_of_datetimes
)
matplotlib.pyplot.plot_date
(
dates
,
values
)
plt.plot
(
[
1,2
]
,
lw=4
,
c=
'
#
8f9805
'
)
from
matplotlib
import
pyplot
as
plt
fig
=
plt.figure
(
)
plt.plot
(
data
)
fig.suptitle
(
'test
title
'
,
fontsize=20
)
plt.xlabel
(
'xlabel
'
,
fontsize=18
)
plt.ylabel
(
'ylabel
'
,
fontsize=16
)
fig.savefig
(
'test.jpg
'
)
import
matplotlib.pyplot
as
plt
#
We
prepare
the
plot
fig
,
ax
=
plt.subplots
(
)
#
We
change
the
fontsize
of
minor
ticks
label
ax.tick_params
(
axis='both
'
,
which='major
'
,
labelsize=10
)
ax.tick_params
(
axis='both
'
,
which='minor
'
,
labelsize=8
)
plt.figure
(
figsize=
(
3
,
4
)
)
df.groupby
(
[
'
A
'
,
'
B
'
]
,
axis=1
)
import
matplotlib.pyplot
as
plt
from
matplotlib.dates
import
date2num
import
datetime
x
=
[
datetime.datetime
(
2011
,
1
,
4
,
0
,
0
)
,
datetime.datetime
(
2011
,
1
,
5
,
0
,
0
)
,
datetime.datetime
(
2011
,
1
,
6
,
0
,
0
)
]
x
=
date2num
(
x
)
y
=
[
4
,
9
,
2
]
z
=
[
1
,
2
,
3
]
k
=
[
11
,
12
,
13
]
ax
=
plt.subplot
(
111
)
ax.bar
(
x-0.2
,
y
,
width=0.2
,
color=
'
b
'
,
align='center
'
)
ax.bar
(
x
,
z
,
width=0.2
,
color=
'
g
'
,
align='center
'
)
ax.bar
(
x+0.2
,
k
,
width=0.2
,
color=
'
r
'
,
align='center
'
)
ax.xaxis_date
(
)
plt.show
(
)
import
matplotlib.pyplot
as
plt
x
=
range
(
10
)
y
=
range
(
10
)
fig
,
ax
=
plt.subplots
(
nrows=2
,
ncols=2
)
for
row
in
ax
:
for
col
in
row
:
col.plot
(
x
,
y
)
plt.show
(
)
from
matplotlib
import
pyplot
as
plt
plt.style.use
(
'ggplot
'
)
for
i
,
v
in
enumerate
(
y
)
:
ax.text
(
v
+
3
,
i
+
.25
,
str
(
v
)
,
color='blue
'
,
fontweight='bold
'
)
import
random
import
matplotlib.pyplot
as
plt
x
=
range
(
1
,
101
)
y1
=
[
random.randint
(
1
,
100
)
for
_
in
xrange
(
len
(
x
)
)
]
y2
=
[
random.randint
(
1
,
100
)
for
_
in
xrange
(
len
(
x
)
)
]
fig
=
plt.figure
(
)
ax
=
fig.add_subplot
(
111
)
#
The
big
subplot
ax1
=
fig.add_subplot
(
211
)
ax2
=
fig.add_subplot
(
212
)
#
Turn
off
axis
lines
and
ticks
of
the
big
subplot
ax.spines
[
'top
'
]
.set_color
(
'none
'
)
ax.spines
[
'bottom
'
]
.set_color
(
'none
'
)
ax.spines
[
'left
'
]
.set_color
(
'none
'
)
ax.spines
[
'right
'
]
.set_color
(
'none
'
)
ax.tick_params
(
labelcolor=
'
w
'
,
top=False
,
bottom=False
,
left=False
,
right=False
)
ax1.loglog
(
x
,
y1
)
ax2.loglog
(
x
,
y2
)
#
Set
common
labels
ax.set_xlabel
(
'common
xlabel
'
)
ax.set_ylabel
(
'common
ylabel
'
)
ax1.set_title
(
'ax1
title
'
)
ax2.set_title
(
'ax2
title
'
)
plt.savefig
(
'common_labels.png
'
,
dpi=300
)
start
,
end
=
ax.get_xlim
(
)
ax.xaxis.set_ticks
(
np.arange
(
start
,
end
,
stepsize
)
)
from
matplotlib
import
pyplot
as
plt
fig
=
plt.figure
(
)
plt.plot
(
data
)
fig.suptitle
(
'test
title
'
,
fontsize=20
)
plt.xlabel
(
'xlabel
'
,
fontsize=18
)
plt.ylabel
(
'ylabel
'
,
fontsize=16
)
fig.savefig
(
'test.jpg
'
)
plt.legend
(
frameon=False
)
random.choice
(
random.choice
(
sample
)
,
repeat=n
)
random.choice
(
random.choice
(
sample
)
,
repeat=n
)
sorted
(
list
(
dict.items
(
)
)
,
key=lambda
x
:
x
[
1
]
)
os.listdir
(
'
<
unk
>
'
)
from
shutil
import
copyfile
copyfile
(
src
,
dst
)
>
>
>
``
Hello\n\n\n
''
.rstrip
(
``
\n
''
)
''
Hello
''
with
open
(
``
test.txt
''
,
``
a
''
)
as
myfile
:
myfile.write
(
``
appended
text
''
)
>
>
>
u8
=
v.decode
(
``
iso-8859-1
''
)
.encode
(
``
utf-8
''
)
>
>
>
u8
'\xc3\x84pple
'
#
convert
iso-8859-1
to
unicode
to
utf-8
>
>
>
tell_me_about
(
u8
)
(
<
type
'str
'
>
,
'\xc3\x84pple
'
)
>
>
>
u16
=
v.decode
(
'iso-8859-1
'
)
.encode
(
'utf-16
'
)
>
>
>
tell_me_about
(
u16
)
(
<
type
'str
'
>
,
'\xff\xfe\xc4\x00p\x00p\x00l\x00e\x00
'
)
>
>
>
tell_me_about
(
u8.decode
(
'utf8
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
tell_me_about
(
u16.decode
(
'utf16
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
shutil.copy2
(
'
<
unk
>
'
,
'
<
unk
>
'
)
datetime.datetime.now
(
)
from
datetime
import
datetime
,
timedelta
d
=
datetime.today
(
)
-
timedelta
(
days=days_to_subtract
)
import
pandas
as
pd
df=pd.read_csv
(
'myfile.csv
'
,
sep=
'
,
'
,
header=None
)
df.values
array
(
[
[
1.
,
2.
,
3
.
]
,
[
4.
,
5.5
,
6
.
]
]
)
df1
=
df.iloc
[
:
,
0:2
]
#
Remember
that
Python
does
not
slice
inclusive
of
the
ending
index
.
df.to_csv
(
file_name
,
sep='\t
'
)
>
>
>
import
os
>
>
>
filename
,
file_extension
=
os.path.splitext
(
'/path/to/somefile.ext
'
)
>
>
>
filename
'/path/to/somefile'
>
>
>
file_extension
'.ext
'
shutil.copy2
(
ddd.png
,
'ddd.png
'
)
shutil.copy2
(
file
,
'en_US
'
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
>
>
>
df
=
pd.DataFrame
(
{
'
a
'
:
[
'red
'
,
'yellow
'
,
'blue
'
]
,
'
b
'
:
[
0.5
,
0.25
,
0.125
]
}
)
>
>
>
df
a
b
0
red
0.500
1
yellow
0.250
2
blue
0.125
df
=
pd.DataFrame
(
{
'oldcol
'
:
[
1,2,3
]
}
)
#
add
column
to
existing
df
df
[
'col
'
]
=
L
print
(
df
)
oldcol
col
0
1
Thanks
You
1
2
Its
fine
no
problem
2
3
Are
you
sure
df
=
pd.DataFrame
(
{
'oldcol
'
:
[
1,2,3
]
}
)
#
add
column
to
existing
df
df
[
'col
'
]
=
L
print
(
df
)
oldcol
col
0
1
Thanks
You
1
2
Its
fine
no
problem
2
3
Are
you
sure
df
=
pd.DataFrame
(
{
'oldcol
'
:
[
1,2,3
]
}
)
#
add
column
to
existing
df
df
[
'col
'
]
=
L
print
(
df
)
oldcol
col
0
1
Thanks
You
1
2
Its
fine
no
problem
2
3
Are
you
sure
df.to_csv
(
``
output.csv
''
,
index=False
)
float
(
'
{
0
:
.2f
}
'.format
(
cast
)
)
list
(
my_dataframe.columns.values
)
df1
=
df1.assign
(
e=pd.Series
(
np.random.randn
(
sLength
)
)
.values
)
pd.DataFrame
(
data
,
columns=
[
'
x
'
,
'
y
'
]
,
axis=1
)
jdf.sort_values
(
by=scores.columns
)
jdf.sort_values
(
by='scores
'
)
from
sklearn.linear_model
import
LogisticRegression
from
sklearn
import
metrics
,
cross_validation
from
sklearn
import
datasets
iris
=
datasets.load_iris
(
)
predicted
=
cross_validation.cross_val_predict
(
LogisticRegression
(
)
,
iris
[
'data
'
]
,
iris
[
'target
'
]
,
cv=10
)
print
metrics.accuracy_score
(
iris
[
'target
'
]
,
predicted
)
Out
[
1
]
:
0.9537
print
metrics.classification_report
(
iris
[
'target
'
]
,
predicted
)
Out
[
2
]
:
precision
recall
f1-score
support
0
1.00
1.00
1.00
50
1
0.96
0.90
0.93
50
2
0.91
0.96
0.93
50
avg
/
total
0.95
0.95
0.95
150
from
sklearn.svm
import
SVC
from
sklearn.datasets
import
load_iris
from
sklearn.model_selection
import
cross_validate
iris
=
load_iris
(
)
scoring
=
[
'precision
'
,
'recall
'
,
'f1
'
]
clf
=
SVC
(
kernel='linear
'
,
C=1
,
random_state=0
)
scores
=
cross_validate
(
clf
,
iris.data
,
iris.target
==
1
,
cv=5
,
scoring=scoring
,
return_train_score=False
)
>
>
>
numpy.random.seed
(
0
)
;
numpy.random.rand
(
4
)
array
(
[
0.55
,
0.72
,
0.6
,
0.54
]
)
>
>
>
numpy.random.seed
(
0
)
;
numpy.random.rand
(
4
)
array
(
[
0.55
,
0.72
,
0.6
,
0.54
]
)
datetime.today
(
)
.strftime
(
'
%
Y-
%
m-
%
d-
%
H
:
%
M
:
%
S
'
)
shutil.copy2
(
file
,
'rb
'
)
import
codecs
with
codecs.open
(
'unicode.rst
'
,
encoding='utf-8
'
)
as
f
:
for
line
in
f
:
print
repr
(
line
)
>
>
>
import
string
>
>
>
import
random
>
>
>
def
id_generator
(
size=6
,
chars=string.ascii_uppercase
+
string.digits
)
:
...
return
``
.join
(
random.choice
(
chars
)
for
_
in
range
(
size
)
)
...
>
>
>
id_generator
(
)
'G5G74W'
>
>
>
id_generator
(
3
,
``
6793YUIO
''
)
'Y3U
'
>
>
>
import
string
>
>
>
import
random
>
>
>
def
id_generator
(
size=6
,
chars=string.ascii_uppercase
+
string.digits
)
:
...
return
``
.join
(
random.choice
(
chars
)
for
_
in
range
(
size
)
)
...
>
>
>
id_generator
(
)
'G5G74W'
>
>
>
id_generator
(
3
,
``
6793YUIO
''
)
'Y3U
'
>
>
>
import
string
>
>
>
import
random
>
>
>
def
id_generator
(
size=6
,
chars=string.ascii_uppercase
+
string.digits
)
:
...
return
``
.join
(
random.choice
(
chars
)
for
_
in
range
(
size
)
)
...
>
>
>
id_generator
(
)
'G5G74W'
>
>
>
id_generator
(
3
,
``
6793YUIO
''
)
'Y3U
'
>
>
>
from
collections
import
defaultdict
sorted
(
list
(
dict.items
(
)
)
,
key=lambda
x
:
x
[
1
]
)
print
(
*myList
,
sep='\n
'
)
os.listdir
(
'
<
unk
>
'
)
with
open
(
'file.txt
'
,
'
r
'
)
as
f
:
f
=
f.open
(
'file.txt
'
,
'
r
'
,
encoding='utf-8
'
)
with
open
(
'file.txt
'
,
'
r
'
)
as
f
:
f
=
f.open
(
'file.txt
'
,
'
r
'
,
encoding='utf-8
'
)
from
shutil
import
copyfile
copyfile
(
src
,
dst
)
s
=
s.lstrip
(
)
df
=
pd.read_csv
(
'myfile.txt
'
,
sep=
'
,
'
,
header=None
)
In
[
30
]
:
df.dropna
(
subset=
[
1
]
)
#
Drop
only
if
NaN
in
specific
column
(
as
asked
in
the
question
)
Out
[
30
]
:
0
1
2
1
2.677677
-1.466923
-0.750366
2
NaN
0.798002
-0.906038
3
0.672201
0.964789
NaN
5
-1.250970
0.030561
-2.678622
6
NaN
1.036043
NaN
7
0.049896
-0.308003
0.823295
9
-0.310130
0.078891
NaN
In
[
36
]
:
DataFrame
(
{
'count
'
:
df1.groupby
(
[
``
Name
''
,
``
City
''
]
)
.size
(
)
}
)
.reset_index
(
)
Out
[
36
]
:
Name
City
count
0
Alice
Seattle
1
1
Bob
Seattle
2
2
Mallory
Portland
2
3
Mallory
Seattle
1
In
[
36
]
:
DataFrame
(
{
'count
'
:
df1.groupby
(
[
``
Name
''
,
``
City
''
]
)
.size
(
)
}
)
.reset_index
(
)
Out
[
36
]
:
Name
City
count
0
Alice
Seattle
1
1
Bob
Seattle
2
2
Mallory
Portland
2
3
Mallory
Seattle
1
df.to_csv
(
'
<
unk
>
'
,
index=False
)
df.groupby
(
np.arange
(
len
(
df.columns
)
)
//
3
)
.mean
(
)
with
open
(
'foo.csv
'
,
'
w
'
)
as
f
:
data.to_csv
(
f
,
index=True
,
header=True
,
decimal=
'
,
'
,
sep=
'
'
,
float_format=
'
%
.3f
'
)
df.sort_values
(
by='*
'
)
df.sort_values
(
by='*
'
)
np.random.seed
(
0
)
df
=
pd.DataFrame
(
{
``
a
''
:
np.random.random_integers
(
1
,
high=100
,
size=100
)
}
)
ranges
=
[
0,10,20,30,40,50,60,70,80,90,100
]
df.groupby
(
pd.cut
(
df.a
,
ranges
)
)
.count
(
)
a
a
(
0
,
10
]
11
(
10
,
20
]
10
(
20
,
30
]
8
(
30
,
40
]
13
(
40
,
50
]
11
(
50
,
60
]
9
(
60
,
70
]
10
(
70
,
80
]
11
(
80
,
90
]
13
(
90
,
100
]
4
df.rename
(
columns=
{
'row
'
:
'row
'
}
,
inplace=True
)
from
matplotlib
import
pyplot
as
plt
import
matplotlib.pyplot
as
plt1
print
(
dir
(
plt
)
==
dir
(
plt1
)
)
True
from
matplotlib
import
pyplot
as
plt
import
matplotlib.pyplot
as
plt1
print
(
dir
(
plt
)
==
dir
(
plt1
)
)
True
f
,
axs
=
plt.subplots
(
2,2
,
figsize=
(
15,15
)
)
fig.add_subplot
(
1
,
1
,
1
)
df.loc
[
df
[
'column_name
'
]
==
some_value
]
os.path.chr
(
)
.encode
(
'utf-8
'
)
#
Doing
linear
regression
with
leave
one
out
cross
val
from
sklearn
import
cross_validation
,
linear_model
import
numpy
as
np
#
Including
this
to
remind
you
that
it
is
necessary
to
use
numpy
arrays
rather
#
than
lists
otherwise
you
will
get
an
error
X_digits
=
np.array
(
x
)
Y_digits
=
np.array
(
y
)
loo
=
cross_validation.LeaveOneOut
(
len
(
Y_digits
)
)
regr
=
linear_model.LinearRegression
(
)
scores
=
cross_validation.cross_val_score
(
regr
,
X_digits
,
Y_digits
,
scoring='mean_squared_error
'
,
cv=loo
,
)
#
This
will
print
the
mean
of
the
list
of
errors
that
were
output
and
#
provide
your
metric
for
evaluation
print
scores.mean
(
)
from
random
import
randrange
print
(
randrange
(
10
)
)
''.join
(
random.choices
(
string.ascii_uppercase
+
string.digits
,
k=N
)
)
>
>
>
import
datetime
>
>
>
datetime.datetime.now
(
)
datetime.datetime
(
2009
,
1
,
6
,
15
,
8
,
24
,
78915
)
>
>
>
print
(
datetime.datetime.now
(
)
)
2009-01-06
15:08:24.789150
date_1
=
datetime.datetime.strptime
(
start_date
,
``
%
m/
%
d/
%
y
''
)
end_date
=
date_1
+
datetime.timedelta
(
days=10
)
import
datetime
datetime.datetime.utcnow
(
)
.replace
(
tzinfo=datetime.timezone.utc
)
.isoformat
(
)
>
>
>
2020-03-20T01:31:12.467113+00:00
>
>
>
import
datetime
>
>
>
'
{
0
:
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
}
'.format
(
datetime.datetime.now
(
)
)
'2014-02-07
11:52:21
'
import
csv
with
open
(
``
test.csv
''
,
``
r
''
)
as
f
:
reader
=
csv.reader
(
f
,
delimiter=
''
\t
''
)
for
i
,
line
in
enumerate
(
reader
)
:
print
'line
[
{
}
]
=
{
}
'.format
(
i
,
line
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
from
os
import
listdir
from
os.path
import
isfile
,
join
onlyfiles
=
[
f
for
f
in
listdir
(
mypath
)
if
isfile
(
join
(
mypath
,
f
)
)
]
def
whatisthis
(
s
)
:
if
isinstance
(
s
,
str
)
:
print
``
ordinary
string
''
elif
isinstance
(
s
,
unicode
)
:
print
``
unicode
string
''
else
:
print
``
not
a
string
''
from
shutil
import
copyfile
copyfile
(
src
,
dst
)
datetime.datetime.now
(
)
.time
(
)
datetime.datetime.now
(
)
.date
(
)
datetime.datetime.now
(
)
.time
(
)
s
=
random.choice
(
'
<
unk
>
'
)
df
=
pd.read_csv
(
'data.csv
'
,
sep='data.csv
'
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
data
=
json.loads
(
f
)
import
json
with
open
(
'data.json
'
,
'
w
'
)
as
f
:
json.dump
(
data
,
f
)
np.genfromtxt
(
'
<
unk
>
'
,
delimiter=
'
,
'
)
import
warnings
warnings.filterwarnings
(
'ignore
'
)
for
x
in
range
(
0
,
100
,
2
)
:
print
(
x
)
import
os
arr
=
os.listdir
(
)
print
(
arr
)
>
>
>
[
'
$
RECYCLE.BIN
'
,
'work.txt
'
,
'3ebooks.txt
'
,
'documents
'
]
import
os
arr
=
os.listdir
(
)
print
(
arr
)
>
>
>
[
'
$
RECYCLE.BIN
'
,
'work.txt
'
,
'3ebooks.txt
'
,
'documents
'
]
text_file
=
open
(
``
Output.txt
''
,
``
w
''
)
text_file.write
(
``
Purchase
Amount
:
%
s
''
%
TotalAmount
)
text_file.close
(
)
json.loads
(
json.loads
(
json
)
)
>
>
>
str
(
10
)
'10'
>
>
>
int
(
'10
'
)
10
>
>
>
import
ast
>
>
>
ast.literal_eval
(
``
{
'muffin
'
:
'lolz
'
,
'foo
'
:
'kitty
'
}
''
)
{
'muffin
'
:
'lolz
'
,
'foo
'
:
'kitty
'
}
data
=
json.loads
(
data
)
data
=
json.loads
(
data
)
import
re
EMAIL_REGEX
=
re.compile
(
r
''
...
regex
here
...
''
)
if
not
EMAIL_REGEX.match
(
email
)
:
#
whatever
for
x
in
range
(
0
,
10
)
:
pass
random.choice
(
random.choice
(
seq.random
)
)
[
(
x
+
y
)
for
x
,
y
in
zip
(
hundred_characters
,
hundred_numbers
)
]
import
os
for
root
,
dirs
,
files
in
os.walk
(
``
/mydir
''
)
:
for
file
in
files
:
if
file.endswith
(
``
.txt
''
)
:
print
(
os.path.join
(
root
,
file
)
)
with
open
(
``
file.txt
''
)
as
file_in
:
lines
=
[
]
for
line
in
file_in
:
lines.append
(
line
)
import
os
rootdir
=
'
C
:
/Users/sid/Desktop/test'
for
subdir
,
dirs
,
files
in
os.walk
(
rootdir
)
:
for
file
in
files
:
print
os.path.join
(
subdir
,
file
)
re.findall
(
'
<
unk
>
'
,
'
<
unk
>
'
)
>
>
>
import
re
>
>
>
s=
''
four
digits
1234
five
digits
56789
six
digits
012345
''
>
>
>
re.findall
(
r
''
\D
(
\d
{
5
}
)
\D
''
,
s
)
[
'56789
'
]
import
re
from
datetime
import
datetime
match
=
re.search
(
r'\d
{
4
}
-\d
{
2
}
-\d
{
2
}
'
,
text
)
date
=
datetime.strptime
(
match.group
(
)
,
'
%
Y-
%
m-
%
d
'
)
.date
(
)
import
argparse
parser
=
argparse.ArgumentParser
(
)
parser.add_argument
(
'
--
foo
'
)
sub
=
parser.add_subparsers
(
)
for
i
in
range
(
1,4
)
:
sp
=
sub.add_parser
(
'cmd
%
i
'
%
i
)
sp.add_argument
(
'
--
foo
%
i
'
%
i
)
#
optionals
have
to
be
distinct
rest
=
'
--
foo
0
cmd2
--
foo2
2
cmd3
--
foo3
3
cmd1
--
foo1
1'.split
(
)
#
or
sys.argv
args
=
argparse.Namespace
(
)
while
rest
:
args
,
rest
=
parser.parse_known_args
(
rest
,
namespace=args
)
print
args
,
rest
import
requests
url
=
'http
:
//192.168.3.45:8080/api/v2/event/log'
data
=
{
``
eventType
''
:
``
AAS_PORTAL_START
''
,
``
data
''
:
{
``
uid
''
:
``
hfe3hf45huf33545
''
,
``
aid
''
:
``
1
''
,
``
vid
''
:
``
1
''
}
}
params
=
{
'sessionKey
'
:
'9ebbd0b25760557393a43064a92bae539d962103
'
,
'format
'
:
'xml
'
,
'platformId
'
:
1
}
requests.post
(
url
,
params=params
,
json=data
)
now
=
datetime.now
(
)
datetime.datetime.now
(
)
+
datetime.timedelta
(
days=1
)
week_date.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
'
)
len
(
df.columns
)
output_f.to_csv
(
'output/output.csv
'
,
index=False
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
df.to_csv
(
'your.csv
'
,
index=False
)
os.path.exists
(
output_file
)
r
=
requests.get
(
'https
:
//jsonplaceholder.typicode.com/posts
'
,
params=userid
)
print
(
'
<
unk
>
%
s
,
<
unk
>
'
%
(
1
,
2
)
)
sys.exit
(
)
from
sklearn.model_selection
import
train_test_split
X_train
,
X_test
,
y_train
,
y_test
=
train_test_split
(
X
,
y
,
stratify=y
,
test_size=0.25
)
#
Doing
linear
regression
with
leave
one
out
cross
val
from
sklearn
import
cross_validation
,
linear_model
import
numpy
as
np
#
Including
this
to
remind
you
that
it
is
necessary
to
use
numpy
arrays
rather
#
than
lists
otherwise
you
will
get
an
error
X_digits
=
np.array
(
x
)
Y_digits
=
np.array
(
y
)
loo
=
cross_validation.LeaveOneOut
(
len
(
Y_digits
)
)
regr
=
linear_model.LinearRegression
(
)
scores
=
cross_validation.cross_val_score
(
regr
,
X_digits
,
Y_digits
,
scoring='mean_squared_error
'
,
cv=loo
,
)
#
This
will
print
the
mean
of
the
list
of
errors
that
were
output
and
#
provide
your
metric
for
evaluation
print
scores.mean
(
)
``
``
''
''
''
''
.join
(
choice
(
ascii_uppercase
)
for
i
in
range
(
12
)
)
``
``
''
''
''
''
.join
(
choice
(
ascii_uppercase
)
for
i
in
range
(
12
)
)
import
os
,
errno
try
:
os.makedirs
(
directory
)
except
OSError
as
e
:
if
e.errno
!
=
errno.EEXIST
:
raise
os.path.abspath
(
os.path
)
In
[
146
]
:
import
re
In
[
152
]
:
match
=
re.search
(
r
'
(
\d+/\d+/\d+
)
'
,
'The
date
is
11/12/98
'
)
In
[
153
]
:
match.group
(
1
)
Out
[
153
]
:
'11/12/98
'
import
os
import
errno
filename
=
``
/foo/bar/baz.txt
''
if
not
os.path.exists
(
os.path.dirname
(
filename
)
)
:
try
:
os.makedirs
(
os.path.dirname
(
filename
)
)
except
OSError
as
exc
:
#
Guard
against
race
condition
if
exc.errno
!
=
errno.EEXIST
:
raise
with
open
(
filename
,
``
w
''
)
as
f
:
f.write
(
``
FOOBAR
''
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
>
>
>
datetime.datetime.now
(
)
.time
(
)
datetime.time
(
15
,
8
,
24
,
78915
)
>
>
>
print
(
datetime.datetime.now
(
)
.time
(
)
)
15:08:24.789150
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
import
csv
def
getstuff
(
filename
,
criterion
)
:
with
open
(
filename
,
``
rb
''
)
as
csvfile
:
datareader
=
csv.reader
(
csvfile
)
yield
next
(
datareader
)
#
yield
the
header
row
count
=
0
for
row
in
datareader
:
if
row
[
3
]
==
criterion
:
yield
row
count
+=
1
elif
count
:
#
done
when
having
read
a
consecutive
series
of
rows
return
with
open
(
'output.csv
'
,
'
w
'
,
newline=
''
)
as
f
:
writer
=
csv.writer
(
f
)
...
import
csv
with
open
(
``
test.csv
''
,
``
r
''
)
as
f
:
reader
=
csv.reader
(
f
,
delimiter=
''
\t
''
)
for
i
,
line
in
enumerate
(
reader
)
:
print
'line
[
{
}
]
=
{
}
'.format
(
i
,
line
)
#
#
text=List
of
strings
to
be
written
to
file
with
open
(
'csvfile.csv
'
,
'wb
'
)
as
file
:
for
line
in
text
:
file.write
(
line
)
file.write
(
'\n
'
)
import
csv
with
open
(
<
path
to
output_csv
>
,
``
wb
''
)
as
csv_file
:
writer
=
csv.writer
(
csv_file
,
delimiter=
'
,
'
)
for
line
in
data
:
writer.writerow
(
line
)
try
:
os.makedirs
(
``
path/to/directory
''
)
except
FileExistsError
:
#
directory
already
exists
pass
import
fnmatch
import
os
matches
=
[
]
for
root
,
dirnames
,
filenames
in
os.walk
(
'src
'
)
:
for
filename
in
fnmatch.filter
(
filenames
,
'*.c
'
)
:
matches.append
(
os.path.join
(
root
,
filename
)
)
import
csv
with
open
(
'file.csv
'
,
newline=
''
)
as
f
:
reader
=
csv.reader
(
f
)
data
=
list
(
reader
)
print
(
data
)
#
Python
2
with
open
(
'/pythonwork/thefile_subset11.csv
'
,
'wb
'
)
as
outfile
:
writer
=
csv.writer
(
outfile
)
#
Python
3
with
open
(
'/pythonwork/thefile_subset11.csv
'
,
'
w
'
,
newline=
''
)
as
outfile
:
writer
=
csv.writer
(
outfile
)
random.randint
(
0
,
9
)
print
(
'
.
'
,
end=
''
)
os.chdir
(
'data
'
)
import
os
for
file
in
os.listdir
(
``
/mydir
''
)
:
if
file.endswith
(
``
.txt
''
)
:
print
(
os.path.join
(
``
/mydir
''
,
file
)
)
import
codecs
with
codecs.open
(
filename
,
'
r
'
,
encoding='utf8
'
)
as
f
:
text
=
f.read
(
)
#
process
Unicode
text
with
codecs.open
(
filename
,
'
w
'
,
encoding='utf8
'
)
as
f
:
f.write
(
text
)
from
shutil
import
copyfile
copyfile
(
src
,
dst
)
os.path.exists
(
)
from
sklearn.datasets
import
load_iris
iris
=
load_iris
(
)
#
`
iris.data
`
holds
the
numerical
values
#
`
iris.feature_names
`
holds
the
numerical
column
names
#
`
iris.target
`
holds
the
categorical
(
species
)
values
(
as
ints
)
#
`
iris.target_names
`
holds
the
unique
categorical
names
from
sklearn.ensemble
import
RandomForestClassifier
from
sklearn.model_selection
import
cross_val_score
import
numpy
as
np
clf
=
RandomForestClassifier
(
)
#
Initialize
with
whatever
parameters
you
want
to
#
10-Fold
Cross
validation
print
np.mean
(
cross_val_score
(
clf
,
X_train
,
y_train
,
cv=10
)
)
import
pandas
as
pd
print
pd.read_csv
(
'value.txt
'
)
Date
price
factor_1
factor_2
0
2012-06-11
1600.20
1.255
1.548
1
2012-06-12
1610.02
1.258
1.554
2
2012-06-13
1618.07
1.249
1.552
3
2012-06-14
1624.40
1.253
1.556
4
2012-06-15
1626.15
1.258
1.552
5
2012-06-16
1626.15
1.263
1.558
6
2012-06-17
1626.15
1.264
1.572
df.groupby
(
'User
'
)
.agg
(
{
'
A
'
:
x.mean
}
)
import
numpy
as
np
import
matplotlib.pyplot
as
plt
#
makes
the
data
y1
=
np.random.normal
(
-2
,
2
,
1000
)
y2
=
np.random.normal
(
2
,
2
,
5000
)
colors
=
[
'
b
'
,
'
g
'
]
#
plots
the
histogram
fig
,
ax1
=
plt.subplots
(
)
ax1.hist
(
[
y1
,
y2
]
,
color=colors
)
ax1.set_xlim
(
-10,10
)
ax1.set_ylabel
(
``
Count
''
)
plt.tight_layout
(
)
plt.show
(
)
import
numpy
as
np
import
matplotlib.pyplot
as
plt
N
=
3
ind
=
np.arange
(
N
)
#
the
x
locations
for
the
groups
width
=
0.27
#
the
width
of
the
bars
fig
=
plt.figure
(
)
ax
=
fig.add_subplot
(
111
)
yvals
=
[
4
,
9
,
2
]
rects1
=
ax.bar
(
ind
,
yvals
,
width
,
color=
'
r
'
)
zvals
=
[
1,2,3
]
rects2
=
ax.bar
(
ind+width
,
zvals
,
width
,
color=
'
g
'
)
kvals
=
[
11,12,13
]
rects3
=
ax.bar
(
ind+width*2
,
kvals
,
width
,
color=
'
b
'
)
ax.set_ylabel
(
'Scores
'
)
ax.set_xticks
(
ind+width
)
ax.set_xticklabels
(
(
'2011-Jan-4
'
,
'2011-Jan-5
'
,
'2011-Jan-6
'
)
)
ax.legend
(
(
rects1
[
0
]
,
rects2
[
0
]
,
rects3
[
0
]
)
,
(
'
y
'
,
'
z
'
,
'
k
'
)
)
def
autolabel
(
rects
)
:
for
rect
in
rects
:
h
=
rect.get_height
(
)
ax.text
(
rect.get_x
(
)
+rect.get_width
(
)
/2.
,
1.05*h
,
'
%
d
'
%
int
(
h
)
,
ha='center
'
,
va='bottom
'
)
autolabel
(
rects1
)
autolabel
(
rects2
)
autolabel
(
rects3
)
plt.show
(
)
df
=
pd.read_csv
(
'
<
unk
>
'
,
header=None
)
import
pandas
as
pd
f=pd.read_csv
(
``
test.csv
''
)
keep_col
=
[
'day
'
,
'month
'
,
'lat
'
,
'long
'
]
new_f
=
f
[
keep_col
]
new_f.to_csv
(
``
newFile.csv
''
,
index=False
)
import
os
os.rename
(
'
a.txt
'
,
'
b.kml
'
)
requests.get
(
html
,
headers=
{
1
}
.__class__
(
)
)
tr.findAll
(
'td
'
,
style='tr
'
)
import
csv
toCSV
=
[
{
'name
'
:
'bob
'
,
'age':25
,
'weight':200
}
,
{
'name
'
:
'jim
'
,
'age':31
,
'weight':180
}
]
keys
=
toCSV
[
0
]
.keys
(
)
with
open
(
'people.csv
'
,
'
w
'
,
newline=
''
)
as
output_file
:
dict_writer
=
csv.DictWriter
(
output_file
,
keys
)
dict_writer.writeheader
(
)
dict_writer.writerows
(
toCSV
)
import
urllib
urllib.urlretrieve
(
``
http
:
//www.gunnerkrigg.com//comics/00000001.jpg
''
,
``
00000001.jpg
''
)
>
>
>
keys
=
[
'
a
'
,
'
b
'
,
'
c
'
]
>
>
>
values
=
[
1
,
2
,
3
]
>
>
>
dictionary
=
dict
(
zip
(
keys
,
values
)
)
>
>
>
print
(
dictionary
)
{
'
a
'
:
1
,
'
b
'
:
2
,
'
c
'
:
3
}
def
merge_two_dicts
(
x
,
y
)
:
z
=
x.copy
(
)
#
start
with
x
's
keys
and
values
z.update
(
y
)
#
modifies
z
with
y
's
keys
and
values
&
returns
None
return
z
sorted
(
var_1
,
key=sorted
)
sorted
(
list
,
key=sorted
)
sorted
(
values
,
key=values.index
)
import
httplib2
from
bs4
import
BeautifulSoup
,
SoupStrainer
http
=
httplib2.Http
(
)
status
,
response
=
http.request
(
'http
:
//www.nytimes.com
'
)
for
link
in
BeautifulSoup
(
response
,
parse_only=SoupStrainer
(
'
a
'
)
)
:
if
link.has_attr
(
'href
'
)
:
print
(
link
[
'href
'
]
)
>
>
>
125650429603636838/
(
2**53
)
13.949999999999999
>
>
>
234042163/
(
2**24
)
13.949999988079071
>
>
>
a
=
13.946
>
>
>
print
(
a
)
13.946
>
>
>
print
(
``
%
.2f
''
%
a
)
13.95
>
>
>
round
(
a,2
)
13.949999999999999
>
>
>
print
(
``
%
.2f
''
%
round
(
a
,
2
)
)
13.95
>
>
>
print
(
``
{
:
.2f
}
''
.format
(
a
)
)
13.95
>
>
>
print
(
``
{
:
.2f
}
''
.format
(
round
(
a
,
2
)
)
)
13.95
>
>
>
print
(
``
{
:
.15f
}
''
.format
(
round
(
a
,
2
)
)
)
13.949999999999999
>
>
>
a
=
[
0
,
1
,
2
,
3
,
4
,
5
,
6
,
7
,
8
,
9
]
>
>
>
del
a
[
-1
]
>
>
>
a
[
0
,
1
,
2
,
3
,
4
,
5
,
6
,
7
,
8
]
dict
(
zip
(
keys
,
values
)
)
random.randint
(
0
,
7
)
from
random
import
randrange
print
(
randrange
(
10
)
)
dict
(
(
k
,
'
:
'
)
for
k
,
v
in
list
(
d.items
(
)
)
)
df.drop
(
'column_name
'
,
axis=1
,
inplace=True
)
plt.savefig
(
'save
'
)
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
``
``
''
time
''
''
''
.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
'
)
now
=
datetime.datetime.now
(
)
.strftime
(
'
%
H
:
%
M
:
%
S
'
)
with
open
(
'data.txt
'
,
'
r
'
)
as
file
:
data
=
file.read
(
)
.replace
(
'\n
'
,
``
)
data_uri
=
open
(
'11.png
'
,
'rb
'
)
.read
(
)
.encode
(
'base64
'
)
.replace
(
'\n
'
,
``
)
img_tag
=
'
<
img
src=
''
data
:
image/png
;
base64
,
{
0
}
''
>
'.format
(
data_uri
)
print
(
img_tag
)
df.groupby
(
np.arange
(
len
(
mean
)
)
//
2
+
1
,
axis=1
)
.mean
(
)
pd.read_csv
(
'
<
unk
>
'
,
sep=
''
)
df.to_csv
(
file_name
,
sep='\t
'
)
numbers
=
list
(
zip
(
characters
,
numbers
)
)
datetime.datetime.now
(
)
.strftime
(
'
%
Y-
%
m-
%
d
'
)
a
=
[
a_list
]
random.randint
(
0
,
7
)
>
>
>
import
string
>
>
>
string.ascii_letters
'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
>
>
>
import
random
>
>
>
random.choice
(
string.ascii_letters
)
'
j
'
s.sort
(
key
=
operator.itemgetter
(
1
,
2
)
)
import
csv
reader
=
csv.reader
(
open
(
'output.csv
'
,
'rb
'
)
)
reader1
=
csv.reader
(
open
(
'output1.csv
'
,
'rb
'
)
)
writer
=
csv.writer
(
open
(
'appended_output.csv
'
,
'wb
'
)
)
for
row
in
reader
:
row1
=
reader1.next
(
)
writer.writerow
(
row
+
row1
)
import
csv
with
open
(
...
,
'wb
'
)
as
myfile
:
wr
=
csv.writer
(
myfile
,
quoting=csv.QUOTE_ALL
)
wr.writerow
(
mylist
)
writer.writerow
(
write
)
mmap.write
(
'\n
'
)
del
r
[
2
]
wtr.writerow
(
r
)
pd.read_csv
(
'
<
unk
>
'
,
sep=
'
,
'
,
header=None
)
df
=
df.drop
(
'column_name
'
,
1
)
from
os
import
walk
f
=
[
]
for
(
dirpath
,
dirnames
,
filenames
)
in
walk
(
mypath
)
:
f.extend
(
filenames
)
break
import
os
arr
=
os.listdir
(
)
print
(
arr
)
>
>
>
[
'
$
RECYCLE.BIN
'
,
'work.txt
'
,
'3ebooks.txt
'
,
'documents
'
]
re.findall
(
'\\
[
(
.*
?
)
\\
]
'
,
s
)
import
re
from
datetime
import
datetime
match
=
re.search
(
r'\d
{
4
}
-\d
{
2
}
-\d
{
2
}
'
,
text
)
date
=
datetime.strptime
(
match.group
(
)
,
'
%
Y-
%
m-
%
d
'
)
.date
(
)
import
re
line
=
re.sub
(
r
''
<
/
?
\
[
\d+
>
''
,
``
''
,
line
)
import
datetime
t
=
datetime.datetime
(
2012
,
2
,
23
,
0
,
0
)
t.strftime
(
'
%
m/
%
d/
%
Y
'
)
import
shutil
shutil.copy2
(
'/src/dir/file.ext
'
,
'/dst/dir/newname.ext
'
)
#
complete
target
filename
given
shutil.copy2
(
'/src/file.ext
'
,
'/dst/dir
'
)
#
target
filename
is
/dst/dir/file.ext
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
file
=
open
(
'myfile.dat
'
,
'
w+
'
)
import
os
old_file
=
os.path.join
(
``
directory
''
,
``
a.txt
''
)
new_file
=
os.path.join
(
``
directory
''
,
``
b.kml
''
)
os.rename
(
old_file
,
new_file
)
with
open
(
'somefile.txt
'
,
'
a
'
)
as
the_file
:
the_file.write
(
'Hello\n
'
)
df
=
pd.read_csv
(
'my.csv
'
,
header=None
)
import
pandas
as
pd
data_df
=
pd.DataFrame
(
'highfrequency2.csv
'
)
print
(
data_df.columns
)
g
=
df
[
'
b
'
]
.unique
(
)
week_grouped
=
df.groupby
(
'week
'
)
week_grouped.sum
(
)
.reset_index
(
)
.to_csv
(
'week_grouped.csv
'
)
In
[
58
]
:
df.groupby
(
[
'cluster
'
]
)
.mean
(
)
Out
[
58
]
:
time
cluster
1
12.333333
2
54.000000
3
6.000000
In
[
202
]
:
df.groupby
(
'
A
'
)
.agg
(
np.std
,
ddof=0
)
Out
[
202
]
:
B
values
A
1
0.5
2.5
2
0.5
2.5
In
[
203
]
:
df.groupby
(
'
A
'
)
.agg
(
np.std
,
ddof=1
)
Out
[
203
]
:
B
values
A
1
0.707107
3.535534
2
0.707107
3.535534
``
``
''
``
''
''
.rstrip
(
'\r\n
'
)
df.to_csv
(
'pandasfile.csv
'
,
float_format=
'
%
.3f
'
)
df
=
df.drop
(
df.columns
[
[
0
,
1
,
3
]
]
,
axis=1
)
#
df.columns
is
zero-based
pd.Index
re.findall
(
'
<
unk
>
'
,
'раз
'
)
''.join
(
random.choices
(
string.ascii_uppercase
+
string.digits
,
k=N
)
)
print
(
random.randint
(
0
,
50
)
)
letters
=
[
(
x
+
y
)
for
x
,
y
in
zip
(
letters
,
integers
)
]
In
[
1
]
:
import
collections
In
[
2
]
:
d
=
{
2:3
,
1:89
,
4:5
,
3:0
}
In
[
3
]
:
od
=
collections.OrderedDict
(
sorted
(
d.items
(
)
)
)
In
[
4
]
:
od
Out
[
4
]
:
OrderedDict
(
[
(
1
,
89
)
,
(
2
,
3
)
,
(
3
,
0
)
,
(
4
,
5
)
]
)
In
[
1
]
:
import
collections
In
[
2
]
:
d
=
{
2:3
,
1:89
,
4:5
,
3:0
}
In
[
3
]
:
od
=
collections.OrderedDict
(
sorted
(
d.items
(
)
)
)
In
[
4
]
:
od
Out
[
4
]
:
OrderedDict
(
[
(
1
,
89
)
,
(
2
,
3
)
,
(
3
,
0
)
,
(
4
,
5
)
]
)
df
=
pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
,
header=None
)
df
=
df.loc
[
-1
]
df.to_csv
(
file_name
,
sep='\t
'
)
import
os
import
shutil
src_files
=
os.listdir
(
src
)
for
file_name
in
src_files
:
full_file_name
=
os.path.join
(
src
,
file_name
)
if
os.path.isfile
(
full_file_name
)
:
shutil.copy
(
full_file_name
,
dest
)
import
requests
r
=
requests.get
(
``
http
:
//example.com/foo/bar
''
)
r
=
requests.get
(
url
,
verify=False
)
from
BeautifulSoup
import
BeautifulSoup
html
=
``
'
<
a
href=
''
some_url
''
>
next
<
/a
>
<
span
class=
''
class
''
>
<
a
href=
''
another_url
''
>
later
<
/a
>
<
/span
>
'
''
soup
=
BeautifulSoup
(
html
)
for
a
in
soup.find_all
(
'
a
'
,
href=True
)
:
print
``
Found
the
URL
:
''
,
a
[
'href
'
]
for
strong_tag
in
soup.find_all
(
'strong
'
)
:
print
(
strong_tag.text
,
strong_tag.next_sibling
)
soup.find_all
(
``
a
''
,
string=
''
Elsie
''
)
#
[
<
a
href=
''
http
:
//example.com/elsie
''
class=
''
sister
''
id=
''
link1
''
>
Elsie
<
/a
>
]
''.join
(
random.choice
(
string.ascii_uppercase
+
string.digits
)
for
_
in
range
(
N
)
)
print
(
random.randint
(
0
,
100
)
)
random_numbers
=
dict
(
(
key
,
value
)
for
key
,
value
in
list
(
random_letters
.
items
(
)
)
)
print
(
sorted
(
list
(
dic.items
(
)
)
,
key=k
)
)
print
(
'\n'.join
(
str
(
key
)
for
line
in
dic
)
)
df
=
pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
,
header=None
)
df.drop
(
'column_name
'
,
axis=1
,
inplace=True
)
output.to_csv
(
'output
'
,
index=False
)
copyfile
(
src_path
,
dest_path
)
os.path.isfile
(
file
)
final_data.to_csv
(
'price.csv
'
)
>
>
>
import
datetime
>
>
>
datetime.datetime.today
(
)
datetime.datetime
(
2012
,
3
,
23
,
23
,
24
,
55
,
173504
)
>
>
>
datetime.datetime.today
(
)
.weekday
(
)
4
from
datetime
import
datetime
,
timedelta
d
=
datetime.today
(
)
-
timedelta
(
days=days_to_subtract
)
time.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
'
,
gmtime
(
)
)
for
i
,
v
in
enumerate
(
y
)
:
ax.text
(
v
+
3
,
i
+
.25
,
str
(
v
)
,
color='blue
'
,
fontweight='bold
'
)
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
timedelta
(
hours=5
)
-
timedelta
(
hours=2
)
datetime.datetime.combine
(
datetime.datetime.now
(
)
,
datetime.datetime.now
(
)
+
datetime.timedelta
(
days=1
)
)
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
print
(
codecs.open
(
'file.txt
'
,
'
w
'
,
encoding='utf-8
'
)
.decode
(
'utf-8
'
)
)
>
>
>
u8
=
v.decode
(
``
iso-8859-1
''
)
.encode
(
``
utf-8
''
)
>
>
>
u8
'\xc3\x84pple
'
#
convert
iso-8859-1
to
unicode
to
utf-8
>
>
>
tell_me_about
(
u8
)
(
<
type
'str
'
>
,
'\xc3\x84pple
'
)
>
>
>
u16
=
v.decode
(
'iso-8859-1
'
)
.encode
(
'utf-16
'
)
>
>
>
tell_me_about
(
u16
)
(
<
type
'str
'
>
,
'\xff\xfe\xc4\x00p\x00p\x00l\x00e\x00
'
)
>
>
>
tell_me_about
(
u8.decode
(
'utf8
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
tell_me_about
(
u16.decode
(
'utf16
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
u8
=
v.decode
(
``
iso-8859-1
''
)
.encode
(
``
utf-8
''
)
>
>
>
u8
'\xc3\x84pple
'
#
convert
iso-8859-1
to
unicode
to
utf-8
>
>
>
tell_me_about
(
u8
)
(
<
type
'str
'
>
,
'\xc3\x84pple
'
)
>
>
>
u16
=
v.decode
(
'iso-8859-1
'
)
.encode
(
'utf-16
'
)
>
>
>
tell_me_about
(
u16
)
(
<
type
'str
'
>
,
'\xff\xfe\xc4\x00p\x00p\x00l\x00e\x00
'
)
>
>
>
tell_me_about
(
u8.decode
(
'utf8
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
tell_me_about
(
u16.decode
(
'utf16
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
import
encodings.idna
x
=
{
1
:
2
,
3
:
4
,
4
:
3
,
2
:
1
,
0
:
0
}
sorted_x
=
sorted
(
x.items
(
)
,
key=lambda
kv
:
kv
[
1
]
)
dict
(
(
k
,
'
:
'
)
for
k
,
v
in
list
(
d.items
(
)
)
)
datetime.datetime.now
(
)
.strftime
(
'
%
a
'
)
datetime.datetime.combine
(
date
,
datetime.time
(
)
)
today
=
datetime.datetime.utcnow
(
)
.date
(
)
now
=
datetime.datetime.utcnow
(
)
.date
(
)
from
datetime
import
datetime
,
timedelta
d
=
datetime.today
(
)
-
timedelta
(
days=days_to_subtract
)
df
=
pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
,
header=None
)
df
=
df.drop
(
'column_name
'
,
1
)
writer.writerow
(
file
)
df
=
pd.read_csv
(
'
<
unk
>
'
,
quotechar=
'
,
'
)
df.to_csv
(
file_name
,
sep='\t
'
)