''.join
(
random.choice
(
string.ascii_uppercase
)
for
_
in
range
(
100
)
)
print
(
datetime.datetime.now
(
)
.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
'
)
)
datetime.datetime.now
(
)
+
datetime.timedelta
(
days=7
)
datetime.datetime.utcnow
(
)
writer.writerow
(
line
)
for
dirname
,
dirnames
,
filenames
in
os.walk
(
'file.txt
'
)
:
for
subdirname
in
dirnames
:
print
(
os.path.join
(
dirname
,
subdirname
)
)
for
filename
in
filenames
:
pass
re.sub
(
'
#
(
\\w+
)
'
,
'\\1
'
,
text
)
pandas.read_csv
(
file
)
df.groupby
(
np.arange
(
len
(
df.columns
)
)
,
axis=1
)
.mean
(
)
df
[
``
column_key
''
]
.mean
(
)
pd.concat
(
[
df1
,
df2
]
,
axis=1
)
In
[
1
]
:
s1
=
pd.Series
(
[
1
,
2
]
,
index=
[
'
A
'
,
'
B
'
]
,
name='s1
'
)
In
[
2
]
:
s2
=
pd.Series
(
[
3
,
4
]
,
index=
[
'
A
'
,
'
B
'
]
,
name='s2
'
)
In
[
3
]
:
pd.concat
(
[
s1
,
s2
]
,
axis=1
)
Out
[
3
]
:
s1
s2
A
1
3
B
2
4
In
[
4
]
:
pd.concat
(
[
s1
,
s2
]
,
axis=1
)
.reset_index
(
)
Out
[
4
]
:
index
s1
s2
0
A
1
3
1
B
2
4
df
=
df.rename
(
columns=
{
'oldName1
'
:
'newName1
'
,
'oldName2
'
:
'newName2
'
}
)
df2
=
df.set_axis
(
[
'
V
'
,
'
W
'
,
'
X
'
,
'
Y
'
,
'
Z
'
]
,
axis=1
,
inplace=False
)
df2
V
W
X
Y
Z
0
x
x
x
x
x
1
x
x
x
x
x
2
x
x
x
x
x
df.to_csv
(
file_name
,
sep='\t
'
)
import
pandas
as
pd
pd.options.display.float_format
=
'
$
{
:
,.2f
}
'.format
df
=
pd.DataFrame
(
[
123.4567
,
234.5678
,
345.6789
,
456.7890
]
,
index=
[
'foo
'
,
'bar
'
,
'baz
'
,
'quux
'
]
,
columns=
[
'cost
'
]
)
print
(
df
)
with
open
(
'foo.csv
'
,
'
w
'
)
as
f
:
data.to_csv
(
f
,
index=True
,
header=True
,
decimal=
'
,
'
,
sep=
'
'
,
float_format=
'
%
.3f
'
)
bins
=
[
0
,
1
,
5
,
10
,
25
,
50
,
100
]
df
[
'binned
'
]
=
pd.cut
(
df
[
'percentage
'
]
,
bins
)
df.loc
[
df
[
'column_name
'
]
==
some_value
]
In
[
13
]
:
df
[
(
df
[
'col1
'
]
>
=
1
)
&
(
df
[
'col1
'
]
<
=1
)
]
Out
[
13
]
:
col1
col2
1
1
11
output
=
df.to_string
(
formatters=
{
'var1
'
:
'
{
:
,.2f
}
'.format
,
'var2
'
:
'
{
:
,.2f
}
'.format
,
'var3
'
:
'
{
:
,.2
%
}
'.format
}
)
print
(
output
)
df
[
'race_label
'
]
=
df.apply
(
lambda
row
:
label_race
(
row
)
,
axis=1
)
LogisticRegression
(
multi_class='multinomial
'
,
solver
='newton-cg
'
)
.fit
(
X_train
,
y_train
)
from
sklearn.ensemble
import
RandomForestClassifier
from
sklearn.model_selection
import
cross_val_score
import
numpy
as
np
clf
=
RandomForestClassifier
(
)
#
Initialize
with
whatever
parameters
you
want
to
#
10-Fold
Cross
validation
print
np.mean
(
cross_val_score
(
clf
,
X_train
,
y_train
,
cv=10
)
)
import
warnings
warnings.filterwarnings
(
'ignore
'
)
plt.savefig
(
'output.png
'
)
dates
=
matplotlib.dates.date2num
(
list_of_datetimes
)
matplotlib.pyplot.plot_date
(
dates
,
values
)
plt.plot
(
[
1,2
]
,
lw=4
,
c='purple
'
)
plt.xlabel
(
'xlabel
'
,
fontsize=18
)
plt.ylabel
(
'ylabel
'
,
fontsize=16
)
ax.tick_params
(
axis='both
'
,
which='major
'
,
labelsize=10
)
plt.figure
(
figsize=
(
3
,
4
)
)
df.groupby
(
[
'
A
'
,
'
B
'
]
,
axis=1
)
ax
=
plt.subplot
(
111
)
ax.bar
(
x-0.2
,
y
,
width=0.2
,
color=
'
b
'
,
align='center
'
)
ax.bar
(
x
,
z
,
width=0.2
,
color=
'
g
'
,
align='center
'
)
ax.bar
(
x+0.2
,
k
,
width=0.2
,
color=
'
r
'
,
align='center
'
)
ax.xaxis_date
(
)
fig
,
ax
=
plt.subplots
(
nrows=2
,
ncols=2
)
plt.style.use
(
'ggplot
'
)
for
i
,
v
in
enumerate
(
y
)
:
ax.text
(
v
+
3
,
i
+
.25
,
str
(
v
)
,
color='blue
'
,
fontweight='bold
'
)
ax.set_xlabel
(
'common
xlabel
'
)
ax.set_ylabel
(
'common
ylabel
'
)
start
,
end
=
ax.get_xlim
(
)
ax.xaxis.set_ticks
(
np.arange
(
start
,
end
,
stepsize
)
)
fig.suptitle
(
'test
title
'
,
fontsize=20
)
plt.legend
(
loc=
''
upper
left
''
)
>
>
>
list
(
map
(
chr
,
range
(
97
,
123
)
)
)
sampled_keys
=
random.choice
(
random.choice
(
l_key
)
,
repeat=100
)
sorted
(
list
(
dictionary.items
(
)
)
,
key=lambda
x
:
x
[
0
]
)
all_files
=
os.listdir
(
'data
'
)
from
shutil
import
copyfile
copyfile
(
os.path.join
(
src_dir
,
filename
)
,
os.path.join
(
tgt_dir
,
filename
)
)
>
>
>
line
=
line.rstrip
(
``
\n
''
)
+
``
\n
''
``
Hello
''
with
open
(
os.path.join
(
tgt_dir
,
filename
)
,
``
w
''
)
as
myfile
:
for
line
in
new_lines
:
myfile.write
(
line
)
line
=
line.decode
(
``
iso-8859-1
''
)
.encode
(
``
utf-8
''
)
shutil.copy2
(
src_filepath
,
tgt_filepath
)
time
=
time.strftime
(
'
%
m-
%
d-
%
Y
%
H
:
%
M
'
)
from
datetime
import
datetime
,
timedelta
time
=
datetime.today
(
)
+
timedelta
(
days=7
)
import
pandas
as
pd
df=pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
)
df1
=
df.iloc
[
:
,
1
:
-1
]
#
Remember
that
Python
does
not
slice
inclusive
of
the
ending
index
.
df1.to_csv
(
``
./example_output/output.csv
''
,
index=False
)
>
>
>
import
os
>
>
>
filename
,
file_extension
=
os.path.splitext
(
'/path/to/somefile.ext
'
)
>
>
>
filename
'/path/to/somefile'
>
>
>
file_extension
'.ext
'
import
shutil
shutil.copy2
(
os.path.join
(
``
./data
''
,
fname
)
,
os.path.join
(
``
./output
''
,
fname
)
)
import
shutil
shutil.copy2
(
os.path.join
(
root
,
fname
)
,
save_dir
)
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
_company_info
=
{
}
for
k
,
v
in
company_info.items
(
)
:
cur_v
=
np.array
(
v
)
_company_info
[
k
]
=
{
'avg_price
'
:
np.mean
(
cur_v
)
,
'max_price
'
:
np.max
(
cur_v
)
,
'min_price
'
:
np.min
(
cur_v
)
,
'stddev_price
'
:
np.std
(
cur_v
)
,
'total_count
'
:
len
(
v
)
}
company_df
=
pd.DataFrame
(
_company_info
)
print
(
company_df
)
company_df.to_csv
(
``
./output/price_by_company.csv
''
)
keys
=
company_info.keys
(
)
company_df
=
pd.DataFrame
(
{
'company
'
:
list
(
keys
)
,
'avg_price
'
:
[
company_info
[
x
]
[
'avg_price
'
]
for
x
in
keys
]
,
'max_price
'
:
[
company_info
[
x
]
[
'max_price
'
]
for
x
in
keys
]
,
'min_price
'
:
[
company_info
[
x
]
[
'min_price
'
]
for
x
in
keys
]
,
'stddev_price
'
:
[
company_info
[
x
]
[
'stddev_price
'
]
for
x
in
keys
]
,
'total_count
'
:
[
company_info
[
x
]
[
'total_count
'
]
for
x
in
keys
]
,
}
)
#
add
column
to
existing
df
df
[
'col
'
]
=
L
print
(
df
)
oldcol
col
0
1
Thanks
You
1
2
Its
fine
no
problem
2
3
Are
you
sure
company_df.to_csv
(
``
output/price_by_company.csv
''
,
index=False
)
avg_price
=
float
(
'
{
0
:
.2f
}
'.format
(
np.mean
(
np.array
(
hp_info
[
key
]
)
)
)
xx
=
list
(
df.columns.values
)
scores.append
(
cur_d
[
``
horsepower
''
]
/
(
cur_d
[
``
average-mileage
''
]
*
cur_d
[
``
price
''
]
)
)
df1
=
df1.assign
(
e=pd.Series
(
np.random.randn
(
sLength
)
)
.values
)
jpd.DataFrame
(
selected_info
,
columns=xx
)
jdf.sort_values
(
by=jdf.columns.scores
)
jdf
=
jdf.sort_values
(
by='scores
'
)
from
sklearn.linear_model
import
LogisticRegression
from
sklearn
import
metrics
,
cross_validation
regressor
=
LogisticRegression
(
c=0.1
)
predicted
=
cross_validation.cross_val_predict
(
regressor
,
X
,
y
,
cv=5
)
print
(
metrics.accuracy_score
(
y
,
predicted
)
)
from
sklearn.model_selection
import
cross_validate
regressor
=
LogisticRegression
(
c=0.1
)
clf
=
SVC
(
kernel='linear
'
,
C=1
,
random_state=0
)
scores
=
cross_validate
(
regressor
,
X
,
y
,
cv=5
)
numpy.random.seed
(
0
)
print
(
now.strftime
(
'
%
m-
%
d-
%
Y
%
H
:
%
M
'
)
)
shutil.copy2
(
in_file
,
out_file
)
import
codecs
with
codecs.open
(
in_file
,
encoding='ISO-8859-15
'
)
as
in_f
:
from
collections
import
defaultdict
from
collections
import
defaultdict
import
string
import
random
from
collections
import
defaultdict
keySorted
=
sorted
(
list
(
dic.keys
(
)
)
)
print
(
key
,
*dic
[
key
]
,
sep=
'
'
)
import
os
files
=
os.listdir
(
'data
'
)
with
codecs.open
(
'data/
'
+
file
,
'
r
'
,
encoding='ISO-8859-15
'
)
as
f
:
with
codecs.open
(
'output/
'
+
file
,
'
w
'
,
encoding='UTF-8
'
)
as
w
:
for
line
in
f.readlines
(
)
:
w.write
(
line
)
with
codecs.open
(
'data/
'
+
file
,
'
r
'
,
encoding='ISO-8859-15
'
)
as
f
:
with
codecs.open
(
'output/
'
+
file
,
'
w
'
,
encoding='UTF-8
'
)
as
w
:
for
line
in
f.readlines
(
)
:
w.write
(
line
)
copyfile
(
'data/
'
+
file
,
'output/
'
+
file
)
continue
line
=
line.lstrip
(
)
import
pandas
as
pd
df
=
pd.read_csv
(
'automobile_data.csv
'
,
sep=
'
,
'
)
df.dropna
(
)
#
Drop
only
if
NaN
in
specific
column
(
as
asked
in
the
question
)
df.to_csv
(
``
output/price_by_company
''
)
pd.DataFrame
(
{
'count
'
:
df.groupby
(
[
``
company
''
]
)
.size
(
)
}
)
.reset_index
(
)
df.to_csv
(
'output/price_by_company
'
,
index=False
)
'avg_price
'
:
group.mean
(
)
,
price_by_company.to_csv
(
'output/price_by_company.csv
'
,
index=False
,
float_format=
'
%
.2f
'
)
price_by_company
=
price_by_company.sort_values
(
by='avg_price
'
,
ascending=False
)
price_by_company
=
price_by_company.sort_values
(
by='avg_price
'
,
ascending=False
)
ranges
=
range
(
0
,
max
(
df.horsepower
)
,
20
)
gp_horsepower
=
df.groupby
(
pd.cut
(
df.horsepower
,
ranges
)
)
price_by_horsepower.rename
(
columns=
{
'horsepower
'
:
'horsepower_range
'
}
,
inplace=True
)
f
,
axs
=
plt.subplots
(
2,2
,
figsize=
(
15,15
)
)
import
matplotlib.pyplot
as
plt
f
,
axs
=
plt.subplots
(
1
,
3
,
figsize=
(
20
,
6
)
)
fig.add_subplot
(
1
,
1
,
1
)
df.loc
[
df
[
'gender
'
]
==
'male
'
]
os.path.chr
(
'output/
'
+
filename
)
.encode
(
'utf-8
'
)
#
Doing
linear
regression
with
leave
one
out
cross
val
from
sklearn
import
cross_validation
,
linear_model
import
numpy
as
np
#
Including
this
to
remind
you
that
it
is
necessary
to
use
numpy
arrays
rather
#
than
lists
otherwise
you
will
get
an
error
X_digits
=
np.array
(
x
)
Y_digits
=
np.array
(
y
)
loo
=
cross_validation.LeaveOneOut
(
len
(
Y_digits
)
)
regr
=
linear_model.LinearRegression
(
)
scores
=
cross_validation.cross_val_score
(
regr
,
X_digits
,
Y_digits
,
scoring='mean_squared_error
'
,
cv=loo
,
)
#
This
will
print
the
mean
of
the
list
of
errors
that
were
output
and
#
provide
your
metric
for
evaluation
print
scores.mean
(
)
from
random
import
randrange
print
(
randrange
(
10
)
)
import
random
import
string
''.join
(
random.choices
(
string.ascii_uppercase
+
string.digits
,
k=N
)
)
import
datetime
datetime.datetime.now
(
)
end_date
=
date_1
+
datetime.timedelta
(
days=7
)
import
datetime
datetime.datetime.utcnow
(
)
.replace
(
tzinfo=datetime.timezone.utc
)
import
datetime
'
{
0
:
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
}
'.format
(
datetime.datetime.now
(
)
)
import
csv
with
open
(
``
test.csv
''
,
``
r
''
)
as
f
:
reader
=
csv.reader
(
f
,
delimiter=
''
,
''
)
for
i
,
line
in
enumerate
(
reader
)
:
print
(
'line
[
{
}
]
=
{
}
'.format
(
i
,
line
)
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
from
os
import
listdir
from
os.path
import
isfile
,
join
onlyfiles
=
[
f
for
f
in
listdir
(
mypath
)
if
isfile
(
join
(
mypath
,
f
)
)
]
def
whatisthis
(
s
)
:
if
isinstance
(
s
,
str
)
:
print
``
ordinary
string
''
elif
isinstance
(
s
,
unicode
)
:
print
``
unicode
string
''
else
:
print
``
not
a
string
''
from
shutil
import
copyfile
copyfile
(
src
,
dst
)
datetime.datetime.now
(
)
.time
(
)
datetime.datetime.now
(
)
.date
(
)
datetime.datetime.now
(
)
.time
(
)
s
=
random.choice
(
[
x
for
x
in
'abcdefghijklmnopqrstuvwxyz
'
]
)
df
=
pd.read_csv
(
'data.csv
'
)
import
os
if
not
os.path.exists
(
'output
'
)
:
os.makedirs
(
'output
'
)
data
=
json.loads
(
f
)
with
open
(
outfile
,
'
w
'
)
as
f
:
json.dump
(
ret
,
f
)
np.genfromtxt
(
'
<
unk
>
'
,
delimiter=
'
,
'
)
import
warnings
warnings.filterwarnings
(
'ignore
'
)
for
x
in
range
(
0
,
100
,
2
)
:
print
(
x
)
>
>
>
s
=
``
Hello
!
''
>
>
>
u
=
unicode
(
s
,
``
utf-8
''
)
``
``
''
''
''
''
.join
(
str
(
c
)
for
c
in
'AndrÃ©
'
)
.decode
(
'utf8
'
)
text_file
=
open
(
``
Output.txt
''
,
``
w
''
)
text_file.write
(
``
Purchase
Amount
:
%
s
''
%
TotalAmount
)
text_file.close
(
)
json.loads
(
json.loads
(
json
)
)
>
>
>
str
(
10
)
'10'
>
>
>
int
(
'10
'
)
10
>
>
>
import
ast
>
>
>
ast.literal_eval
(
``
{
'muffin
'
:
'lolz
'
,
'foo
'
:
'kitty
'
}
''
)
{
'muffin
'
:
'lolz
'
,
'foo
'
:
'kitty
'
}
data
=
json.loads
(
data
)
data
=
json.loads
(
data
)
import
re
EMAIL_REGEX
=
re.compile
(
r
''
...
regex
here
...
''
)
if
not
EMAIL_REGEX.match
(
email
)
:
#
whatever
for
x
in
range
(
count
)
:
pass
random.choice
(
seq
)
zip
(
hundred_characters
,
hundred_numbers
)
import
os
for
root
,
dirs
,
files
in
os.walk
(
``
data
''
)
:
for
file
in
files
:
if
file.endswith
(
``
.txt
''
)
:
print
(
os.path.join
(
root
,
file
)
)
with
open
(
``
file.txt
''
)
as
file_in
:
lines
=
[
]
for
line
in
file_in
:
lines.append
(
line
)
import
os
rootdir
=
'data'
for
subdir
,
dirs
,
files
in
os.walk
(
rootdir
)
:
for
file
in
files
:
print
(
os.path.join
(
subdir
,
file
)
)
re.findall
(
'
<
unk
>
'
,
'
<
unk
>
'
)
re.findall
(
r
''
\d
''
,
s
)
import
re
from
datetime
import
datetime
match
=
re.search
(
r'\d
{
4
}
-\d
{
2
}
-\d
{
2
}
'
,
text
)
date
=
datetime.strptime
(
match.group
(
)
,
'
%
Y-
%
m-
%
d
'
)
.date
(
)
import
argparse
parser
=
argparse.ArgumentParser
(
)
parser.add_argument
(
'
--
foo
'
)
sub
=
parser.add_subparsers
(
)
for
i
in
range
(
1,4
)
:
sp
=
sub.add_parser
(
'cmd
%
i
'
%
i
)
sp.add_argument
(
'
--
foo
%
i
'
%
i
)
#
optionals
have
to
be
distinct
import
requests
url
=
'http
:
//192.168.3.45:8080/api/v2/event/log'
params
=
{
'sessionKey
'
:
'9ebbd0b25760557393a43064a92bae539d962103
'
,
'format
'
:
'xml
'
,
'platformId
'
:
1
}
requests.post
(
url
,
params=params
)
now
=
datetime.now
(
)
datetime.datetime.now
(
)
+
datetime.timedelta
(
days=1
)
week_date.strftime
(
'
%
m-
%
d-
%
Y
%
H
:
%
M
'
)
len
(
df.columns
)
output_f.to_csv
(
'output/output.csv
'
,
sep=
'
,
'
)
import
os
if
os.path.exists
(
``
output
''
)
:
df.to_csv
(
'your.csv
'
,
index=False
)
os.path.exists
(
output_file
)
r
=
requests.get
(
'https
:
//jsonplaceholder.typicode.com/posts
'
,
params=userid
)
print
(
'
<
unk
>
%
s
,
<
unk
>
'
%
(
1
,
2
)
)
sys.exit
(
)
from
sklearn.model_selection
import
train_test_split
X_train
,
X_test
,
y_train
,
y_test
=
train_test_split
(
X
,
y
,
stratify=y
,
test_size=0.20
)
#
Doing
linear
regression
with
leave
one
out
cross
val
from
sklearn
import
cross_validation
,
linear_model
import
numpy
as
np
#
Including
this
to
remind
you
that
it
is
necessary
to
use
numpy
arrays
rather
#
than
lists
otherwise
you
will
get
an
error
X_digits
=
np.array
(
x
)
Y_digits
=
np.array
(
y
)
loo
=
cross_validation.LeaveOneOut
(
len
(
Y_digits
)
)
regr
=
linear_model.LinearRegression
(
)
scores
=
cross_validation.cross_val_score
(
regr
,
X_digits
,
Y_digits
,
scoring='mean_squared_error
'
,
cv=loo
,
)
#
This
will
print
the
mean
of
the
list
of
errors
that
were
output
and
#
provide
your
metric
for
evaluation
print
scores.mean
(
)
randint
(
1
,
20
)
randint
(
1
,
20
)
import
os
,
errno
try
:
os.makedirs
(
directory
)
except
OSError
as
e
:
if
e.errno
!
=
errno.EEXIST
:
raise
os.path.abspath
(
os.path
)
In
[
146
]
:
import
re
In
[
152
]
:
match
=
re.search
(
r
'
(
\d+/\d+/\d+
)
'
,
'The
date
is
11/12/98
'
)
In
[
153
]
:
match.group
(
1
)
Out
[
153
]
:
'11/12/98
'
import
os
import
errno
filename
=
``
/foo/bar/baz.txt
''
if
not
os.path.exists
(
os.path.dirname
(
filename
)
)
:
try
:
os.makedirs
(
os.path.dirname
(
filename
)
)
except
OSError
as
exc
:
#
Guard
against
race
condition
if
exc.errno
!
=
errno.EEXIST
:
raise
with
open
(
filename
,
``
w
''
)
as
f
:
f.write
(
``
FOOBAR
''
)
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
>
>
>
datetime.datetime.now
(
)
.time
(
)
datetime.time
(
15
,
8
,
24
,
78915
)
>
>
>
print
(
datetime.datetime.now
(
)
.time
(
)
)
15:08:24.789150
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
import
csv
def
getstuff
(
filename
,
criterion
)
:
with
open
(
filename
,
``
rb
''
)
as
csvfile
:
datareader
=
csv.reader
(
csvfile
)
yield
next
(
datareader
)
#
yield
the
header
row
count
=
0
for
row
in
datareader
:
if
row
[
3
]
==
criterion
:
yield
row
count
+=
1
elif
count
:
#
done
when
having
read
a
consecutive
series
of
rows
return
with
open
(
'output.csv
'
,
'
w
'
,
newline=
''
)
as
f
:
writer
=
csv.writer
(
f
)
...
import
csv
with
open
(
``
test.csv
''
,
``
r
''
)
as
f
:
reader
=
csv.reader
(
f
,
delimiter=
''
\t
''
)
for
i
,
line
in
enumerate
(
reader
)
:
print
'line
[
{
}
]
=
{
}
'.format
(
i
,
line
)
#
#
text=List
of
strings
to
be
written
to
file
with
open
(
'csvfile.csv
'
,
'wb
'
)
as
file
:
for
line
in
text
:
file.write
(
line
)
file.write
(
'\n
'
)
import
csv
with
open
(
<
path
to
output_csv
>
,
``
wb
''
)
as
csv_file
:
writer
=
csv.writer
(
csv_file
,
delimiter=
'
,
'
)
for
line
in
data
:
writer.writerow
(
line
)
try
:
os.makedirs
(
``
path/to/directory
''
)
except
FileExistsError
:
#
directory
already
exists
pass
import
fnmatch
import
os
matches
=
[
]
for
root
,
dirnames
,
filenames
in
os.walk
(
'src
'
)
:
for
filename
in
fnmatch.filter
(
filenames
,
'*.c
'
)
:
matches.append
(
os.path.join
(
root
,
filename
)
)
import
csv
with
open
(
'file.csv
'
,
newline=
''
)
as
f
:
reader
=
csv.reader
(
f
)
data
=
list
(
reader
)
print
(
data
)
#
Python
2
with
open
(
'/pythonwork/thefile_subset11.csv
'
,
'wb
'
)
as
outfile
:
writer
=
csv.writer
(
outfile
)
#
Python
3
with
open
(
'/pythonwork/thefile_subset11.csv
'
,
'
w
'
,
newline=
''
)
as
outfile
:
writer
=
csv.writer
(
outfile
)
print
(
random.randint
(
0
,
9
)
)
print
(
chr
(
i+ord
(
'
a
'
)
)
,
end=
'
'
)
os.chdir
(
'data
'
)
for
file
in
os.listdir
(
``
data
''
)
:
if
file.endswith
(
``
.txt
''
)
:
print
(
os.path.join
(
``
data
''
,
file
)
)
with
codecs.open
(
filename
,
'
r
'
,
encoding='ISO-8859-15
'
)
as
f
:
text
=
f.read
(
)
#
process
Unicode
text
with
codecs.open
(
filename
,
'
w
'
,
encoding='utf8
'
)
as
f
:
f.write
(
text
)
from
shutil
import
copyfile
copyfile
(
os.path.join
(
``
data
''
,
file
)
,
``
output/
''
+file
)
if
not
os.path.exists
(
``
output
''
)
:
os.mkdir
(
``
ouput
''
)
from
sklearn.datasets
import
load_iris
iris
=
load_iris
(
)
#
`
iris.data
`
holds
the
numerical
values
#
`
iris.feature_names
`
holds
the
numerical
column
names
#
`
iris.target
`
holds
the
categorical
(
species
)
values
(
as
ints
)
#
`
iris.target_names
`
holds
the
unique
categorical
names
from
sklearn.ensemble
import
RandomForestClassifier
from
sklearn.model_selection
import
cross_val_score
import
numpy
as
np
clf
=
RandomForestClassifier
(
)
#
Initialize
with
whatever
parameters
you
want
to
#
10-Fold
Cross
validation
print
np.mean
(
cross_val_score
(
clf
,
X_train
,
y_train
,
cv=10
)
)
import
pandas
as
pd
print
pd.read_csv
(
'value.txt
'
)
Date
price
factor_1
factor_2
0
2012-06-11
1600.20
1.255
1.548
1
2012-06-12
1610.02
1.258
1.554
2
2012-06-13
1618.07
1.249
1.552
3
2012-06-14
1624.40
1.253
1.556
4
2012-06-15
1626.15
1.258
1.552
5
2012-06-16
1626.15
1.263
1.558
6
2012-06-17
1626.15
1.264
1.572
df.groupby
(
'User
'
)
#
makes
the
data
colors
=
[
'
b
'
,
'
g
'
]
#
plots
the
histogram
fig
,
(
ax0
,
ax1
,
ax2
)
=
plt.subplots
(
nrows=1
,
ncols=3
)
#
ax1.hist
(
[
y1
,
y2
]
,
color=colors
)
#
ax1.set_xlim
(
-10,10
)
#
ax1.set_ylabel
(
``
Count
''
)
#
plt.tight_layout
(
)
#
plt.show
(
)
import
numpy
as
np
import
matplotlib.pyplot
as
plt
N
=
3
ind
=
np.arange
(
N
)
#
the
x
locations
for
the
groups
width
=
0.27
#
the
width
of
the
bars
fig
=
plt.figure
(
)
ax
=
fig.add_subplot
(
111
)
yvals
=
[
4
,
9
,
2
]
rects1
=
ax.bar
(
ind
,
yvals
,
width
,
color=
'
r
'
)
zvals
=
[
1,2,3
]
rects2
=
ax.bar
(
ind+width
,
zvals
,
width
,
color=
'
g
'
)
kvals
=
[
11,12,13
]
rects3
=
ax.bar
(
ind+width*2
,
kvals
,
width
,
color=
'
b
'
)
ax.set_ylabel
(
'Scores
'
)
ax.set_xticks
(
ind+width
)
ax.set_xticklabels
(
(
'2011-Jan-4
'
,
'2011-Jan-5
'
,
'2011-Jan-6
'
)
)
ax.legend
(
(
rects1
[
0
]
,
rects2
[
0
]
,
rects3
[
0
]
)
,
(
'
y
'
,
'
z
'
,
'
k
'
)
)
def
autolabel
(
rects
)
:
for
rect
in
rects
:
h
=
rect.get_height
(
)
ax.text
(
rect.get_x
(
)
+rect.get_width
(
)
/2.
,
1.05*h
,
'
%
d
'
%
int
(
h
)
,
ha='center
'
,
va='bottom
'
)
autolabel
(
rects1
)
autolabel
(
rects2
)
autolabel
(
rects3
)
plt.show
(
)
df
=
pd.read_csv
(
'data.csv
'
,
header=None
)
print
(
df
)
import
pandas
as
pd
f=pd.read_csv
(
``
data.csv
''
)
keep_col
=
[
'day
'
,
'month
'
,
'lat
'
,
'long
'
]
new_f
=
f
[
keep_col
]
new_f.to_csv
(
``
newFile.csv
''
,
index=False
)
os.rename
(
'
a.txt
'
,
'
b.kml
'
)
import
requests
url
=
'https
:
//en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'
x
=
requests.get
(
url
)
print
(
x
)
tr.findAll
(
'td
'
,
style='tr
'
)
import
csv
toCSV
=
[
{
'name
'
:
'bob
'
,
'age':25
,
'weight':200
}
,
{
'name
'
:
'jim
'
,
'age':31
,
'weight':180
}
]
keys
=
toCSV
[
0
]
.keys
(
)
with
open
(
'people.csv
'
,
'
w
'
,
newline=
''
)
as
output_file
:
dict_writer
=
csv.DictWriter
(
output_file
,
keys
)
dict_writer.writeheader
(
)
dict_writer.writerows
(
toCSV
)
urllib.urlretrieve
(
``
http
:
//www.gunnerkrigg.com//comics/00000001.jpg
''
,
``
00000001.jpg
''
)
dictionary
=
dict
(
zip
(
letters
,
numbers
)
)
print
(
dictionary
)
print
(
sorted
(
values
,
key=sorted
)
print
(
sorted
(
values
,
key=values.index
)
)
http
=
httplib2.Http
(
)
status
,
response
=
http.request
(
'https
:
//frankxfz.me/snapshot.html
'
)
for
link
in
BeautifulSoup
(
response
,
parse_only=SoupStrainer
(
'
a
'
)
)
:
if
link.has_attr
(
'href
'
)
:
print
(
link
[
'href
'
]
)
del
companies
[
n
]
del
avg_price
[
n
]
del
max_price
[
n
]
del
min_price
[
n
]
del
cars
[
n
]
dict
=
zip
(
index
,
scores
)
random.randint
(
0
,
7
)
from
random
import
randrange
print
(
randrange
(
10
)
)
dict
(
(
k
,
'
:
'
)
for
k
,
v
in
list
(
d.items
(
)
)
)
df.drop
(
'column_name
'
,
axis=1
,
inplace=True
)
plt.savefig
(
'save
'
)
from
time
import
gmtime
,
strftime
print
(
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
)
``
``
''
time
''
''
''
.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
'
)
now
=
datetime.datetime.now
(
)
.strftime
(
'
%
H
:
%
M
:
%
S
'
)
with
open
(
'data.txt
'
,
'
r
'
)
as
file
:
data
=
file.read
(
)
.replace
(
'\n
'
,
``
)
data_uri
=
open
(
'11.png
'
,
'rb
'
)
.read
(
)
.encode
(
'base64
'
)
.replace
(
'\n
'
,
``
)
img_tag
=
'
<
img
src=
''
data
:
image/png
;
base64
,
{
0
}
''
>
'.format
(
data_uri
)
print
(
img_tag
)
df.groupby
(
np.arange
(
len
(
mean
)
)
//
2
+
1
,
axis=1
)
.mean
(
)
pd.read_csv
(
'
<
unk
>
'
,
sep=
''
)
df.to_csv
(
file_name
,
sep='\t
'
)
pairs
=
list
(
zip
(
characters
,
numbers
)
)
datetime.datetime.now
(
)
.strftime
(
'
%
Y-
%
m-
%
d
'
)
a
=
[
a_list
]
random.randint
(
0
,
7
)
>
>
>
import
string
>
>
>
string.ascii_letters
'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
>
>
>
import
random
>
>
>
random.choice
(
string.ascii_letters
)
'
j
'
s.sort
(
key
=
operator.itemgetter
(
1
,
2
)
)
import
csv
reader
=
csv.reader
(
open
(
'output.csv
'
,
'rb
'
)
)
reader1
=
csv.reader
(
open
(
'output1.csv
'
,
'rb
'
)
)
writer
=
csv.writer
(
open
(
'appended_output.csv
'
,
'wb
'
)
)
for
row
in
reader
:
row1
=
reader1.next
(
)
writer.writerow
(
row
+
row1
)
import
csv
with
open
(
...
,
'wb
'
)
as
myfile
:
wr
=
csv.writer
(
myfile
,
quoting=csv.QUOTE_ALL
)
wr.writerow
(
mylist
)
writer.writerow
(
write
)
mmap.write
(
'\n
'
)
del
r
[
2
]
wtr.writerow
(
r
)
pd.read_csv
(
'
<
unk
>
'
,
sep=
'
,
'
,
header=None
)
df
=
df.drop
(
'column_name
'
,
1
)
from
os
import
walk
f
=
[
]
for
(
dirpath
,
dirnames
,
filenames
)
in
walk
(
mypath
)
:
f.extend
(
filenames
)
break
import
os
arr
=
os.listdir
(
)
print
(
arr
)
>
>
>
[
'
$
RECYCLE.BIN
'
,
'work.txt
'
,
'3ebooks.txt
'
,
'documents
'
]
re.findall
(
'\\
[
(
.*
?
)
\\
]
'
,
s
)
import
re
from
datetime
import
datetime
match
=
re.search
(
r'\d
{
4
}
-\d
{
2
}
-\d
{
2
}
'
,
text
)
date
=
datetime.strptime
(
match.group
(
)
,
'
%
Y-
%
m-
%
d
'
)
.date
(
)
import
re
line
=
re.sub
(
r
''
<
/
?
\
[
\d+
>
''
,
``
''
,
line
)
import
datetime
t
=
datetime.datetime
(
2012
,
2
,
23
,
0
,
0
)
t.strftime
(
'
%
m/
%
d/
%
Y
'
)
import
shutil
shutil.copy2
(
'/src/dir/file.ext
'
,
'/dst/dir/newname.ext
'
)
#
complete
target
filename
given
shutil.copy2
(
'/src/file.ext
'
,
'/dst/dir
'
)
#
target
filename
is
/dst/dir/file.ext
import
os
if
not
os.path.exists
(
directory
)
:
os.makedirs
(
directory
)
file
=
open
(
'myfile.dat
'
,
'
w+
'
)
import
os
old_file
=
os.path.join
(
``
directory
''
,
``
a.txt
''
)
new_file
=
os.path.join
(
``
directory
''
,
``
b.kml
''
)
os.rename
(
old_file
,
new_file
)
with
open
(
'somefile.txt
'
,
'
a
'
)
as
the_file
:
the_file.write
(
'Hello\n
'
)
df
=
pd.read_csv
(
'my.csv
'
,
header=None
)
import
pandas
as
pd
data_df
=
pd.DataFrame
(
'highfrequency2.csv
'
)
print
(
data_df.columns
)
g
=
df
[
'
b
'
]
.unique
(
)
week_grouped
=
df.groupby
(
'week
'
)
week_grouped.sum
(
)
.reset_index
(
)
.to_csv
(
'week_grouped.csv
'
)
In
[
58
]
:
df.groupby
(
[
'cluster
'
]
)
.mean
(
)
Out
[
58
]
:
time
cluster
1
12.333333
2
54.000000
3
6.000000
In
[
202
]
:
df.groupby
(
'
A
'
)
.agg
(
np.std
,
ddof=0
)
Out
[
202
]
:
B
values
A
1
0.5
2.5
2
0.5
2.5
In
[
203
]
:
df.groupby
(
'
A
'
)
.agg
(
np.std
,
ddof=1
)
Out
[
203
]
:
B
values
A
1
0.707107
3.535534
2
0.707107
3.535534
``
``
''
``
''
''
.rstrip
(
'\r\n
'
)
df.to_csv
(
'pandasfile.csv
'
,
float_format=
'
%
.3f
'
)
df
=
df.drop
(
df.columns
[
[
0
,
1
,
3
]
]
,
axis=1
)
#
df.columns
is
zero-based
pd.Index
re.findall
(
'
<
unk
>
'
,
'раз
'
)
import
string
import
random
''.join
(
random.choices
(
string.ascii_lowercase
,
k=1
)
)
print
(
random.randint
(
1,20
)
)
dic
=
{
}
for
i
in
range
(
100
)
:
letter
=
letters
[
i
]
num
=
integers
[
i
]
if
letter
in
dic
:
dic
[
letter
]
.append
(
num
)
else
:
dic
[
letter
]
=
[
num
]
print
(
dic
)
In
[
1
]
:
import
collections
In
[
2
]
:
d
=
{
2:3
,
1:89
,
4:5
,
3:0
}
In
[
3
]
:
od
=
collections.OrderedDict
(
sorted
(
d.items
(
)
)
)
In
[
4
]
:
od
Out
[
4
]
:
OrderedDict
(
[
(
1
,
89
)
,
(
2
,
3
)
,
(
3
,
0
)
,
(
4
,
5
)
]
)
for
key
in
sorted
(
dic.keys
(
)
)
:
print
(
key
,
'
'.join
(
map
(
str
,
sorted
(
dic
[
key
]
)
)
)
)
import
pandas
as
pd
df
=
pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
,
header=None
)
df
=
df.iloc
[
1
:
len
(
df
)
-1
]
file_name
=
'output/output.csv'
df.to_csv
(
file_name
,
sep=
'
,
'
)
from
distutils.dir_util
import
copy_tree
src
=
'data'
dest
=
'output'
#
copy
subdirectory
example
fromDirectory
=
src
toDirectory
=
dest
copy_tree
(
fromDirectory
,
toDirectory
)
import
requests
r
=
requests.get
(
``
http
:
//example.com/foo/bar
''
)
import
requests
r
=
requests.get
(
url
)
for
a
in
soup.find_all
(
'
a
'
,
href=True
)
:
print
``
Found
the
URL
:
''
,
a
[
'href
'
]
for
strong_tag
in
soup.find_all
(
'
b
'
)
:
print
(
strong_tag.text
)
for
ita
in
soup.find_all
(
'span
'
)
:
print
(
ita.text
)
#
[
<
a
href=
''
http
:
//example.com/elsie
''
class=
''
sister
''
id=
''
link1
''
>
Elsie
<
/a
>
]
random_letters
=
``
.join
(
random.choice
(
string.ascii_lowercase
)
for
_
in
range
(
100
)
)
print
(
random_letters
)
random_numbers
=
[
random.randint
(
1
,
20
)
for
_
in
range
(
100
)
]
dic
=
dict
(
(
key
,
value
)
for
key
,
value
in
zip
(
random_letters
,
random_numbers
)
)
print
(
sorted
(
list
(
dic.keys
(
)
)
)
)
print
(
'\n'.join
(
str
(
key
)
for
key
in
dic
)
)
df
=
pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
,
header=None
)
output
=
df.drop
(
'id
'
,
axis=1
)
output
=
output.drop
(
'ip_address
'
,
axis=1
)
output.to_csv
(
'output.csv
'
,
index=False
)
copyfile
(
src_path
,
dest_path
)
if
os.path.isfile
(
file
)
:
pass
final_data.to_csv
(
'price.csv
'
)
>
>
>
import
datetime
>
>
>
datetime.datetime.today
(
)
datetime.datetime
(
2012
,
3
,
23
,
23
,
24
,
55
,
173504
)
>
>
>
datetime.datetime.today
(
)
.weekday
(
)
4
from
datetime
import
datetime
,
timedelta
d
=
datetime.today
(
)
+
timedelta
(
days=7
)
time.strftime
(
'
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
'
,
gmtime
(
)
)
for
i
,
v
in
enumerate
(
y
)
:
ax.text
(
v
+
3
,
i
+
.25
,
str
(
v
)
,
color='blue
'
,
fontweight='bold
'
)
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
timedelta
(
hours=5
)
-
timedelta
(
hours=2
)
datetime.datetime.combine
(
datetime.datetime.now
(
)
,
datetime.datetime.now
(
)
+
datetime.timedelta
(
days=1
)
)
>
>
>
from
time
import
gmtime
,
strftime
>
>
>
strftime
(
``
%
Y-
%
m-
%
d
%
H
:
%
M
:
%
S
''
,
gmtime
(
)
)
'2009-01-05
22:14:39
'
print
(
codecs.open
(
'file.txt
'
,
'
w
'
,
encoding='utf-8
'
)
.decode
(
'utf-8
'
)
)
>
>
>
u8
=
v.decode
(
``
iso-8859-1
''
)
.encode
(
``
utf-8
''
)
>
>
>
u8
'\xc3\x84pple
'
#
convert
iso-8859-1
to
unicode
to
utf-8
>
>
>
tell_me_about
(
u8
)
(
<
type
'str
'
>
,
'\xc3\x84pple
'
)
>
>
>
u16
=
v.decode
(
'iso-8859-1
'
)
.encode
(
'utf-16
'
)
>
>
>
tell_me_about
(
u16
)
(
<
type
'str
'
>
,
'\xff\xfe\xc4\x00p\x00p\x00l\x00e\x00
'
)
>
>
>
tell_me_about
(
u8.decode
(
'utf8
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
tell_me_about
(
u16.decode
(
'utf16
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
u8
=
v.decode
(
``
iso-8859-1
''
)
.encode
(
``
utf-8
''
)
>
>
>
u8
'\xc3\x84pple
'
#
convert
iso-8859-1
to
unicode
to
utf-8
>
>
>
tell_me_about
(
u8
)
(
<
type
'str
'
>
,
'\xc3\x84pple
'
)
>
>
>
u16
=
v.decode
(
'iso-8859-1
'
)
.encode
(
'utf-16
'
)
>
>
>
tell_me_about
(
u16
)
(
<
type
'str
'
>
,
'\xff\xfe\xc4\x00p\x00p\x00l\x00e\x00
'
)
>
>
>
tell_me_about
(
u8.decode
(
'utf8
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
>
>
>
tell_me_about
(
u16.decode
(
'utf16
'
)
)
(
<
type
'unicode
'
>
,
u'\xc4pple
'
)
import
encodings.idna
x
=
{
1
:
2
,
3
:
4
,
4
:
3
,
2
:
1
,
0
:
0
}
sorted_x
=
sorted
(
x.items
(
)
,
key=lambda
kv
:
kv
[
1
]
)
dict
(
(
k
,
'
:
'
)
for
k
,
v
in
list
(
d.items
(
)
)
)
date
=
atetime.datetime.now
(
)
.strftime
(
'
%
a
'
)
date
=
datetime.datetime.combine
(
date
datetime.time
(
)
)
today
=
datetime.datetime.utcnow
(
)
.date
(
)
now
=
datetime.datetime.utcnow
(
)
.time
(
)
from
datetime
import
datetime
,
timedelta
d
=
datetime.today
(
)
-
timedelta
(
days=7
)
df
=
pd.read_csv
(
'data.csv
'
,
sep=
'
,
'
,
header=None
)
df
=
df.drop
(
'column_name
'
,
1
)
writer.writerow
(
file
)
df
=
pd.read_csv
(
'
<
unk
>
'
,
quotechar=
'
,
'
)
df.to_csv
(
file_name
,
sep='\t
'
)